{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"Exercises/Ex13/","text":"Exercise 13 Goal The Goal is to automate the creation of a server provided bei Hetzner with terraform. To avoid possible security vulnerabilities originating from login with password, our created server should only be accessed via SSH-key login . Important information such as the IP-address, which is necessary to connect to the server, has to be returned in the console after creating it. Additionally, the server configuration must be tracked with git . Steps 1. Automation The first step is to set up a main.tf file with minimal terraform configuration, including the Cloud Provider Hetzner, the API token and the initial creation of the server. Running this file with $ terraform apply will start up a Server provided by Hetzner. # Define Hetzner cloud provider terraform { required_providers { hcloud = { source = \"hetznercloud/hcloud\" } } required_version = \">= 0.13\" } # Configure the Hetzner Cloud API token provider \"hcloud\" { token = var.hcloud_token } # Create a server resource \"hcloud_server\" \"mainServer\" { name = \"main-server\" image = \"debian-13\" server_type = \"cx23\" } The variable \"token\" is defined in a different file called variables.tf. variable \"hcloud_token\" { nullable = false sensitive = true } TODO: explain how to get and set token as environment variable 2. Login via SSH-key To ensure that the created server can only be accessed via SSH key authentication, the main.tf file has to contain a firewall configuration which only allows port 22 (SSH). resource \"hcloud_firewall\" \"sshFw\" { name = \"ssh-firewall\" rule { direction = \"in\" protocol = \"tcp\" port = \"22\" source_ips = [\"0.0.0.0/0\", \"::/0\"] } } Following the setup of the firewall, the public keys of the authorized users must be added to main.tf. The public/private key pairs were previously created with $ ssh-keygen -a 256 -t ed25519 . #SSH-Keys resource \"hcloud_ssh_key\" \"loginUser1\" { name = \"se109@hdm-stuttgart.de\" public_key = file(\"../ssh/id_ed25519.pub\") } resource \"hcloud_ssh_key\" \"loginUser2\" { name = \"jb321@hdm-stuttgart.de\" public_key = file(\"../ssh/id_ed25519_infra.pub\") } The block where the server is created has to be modified to contain the SSH-keys. # Create a server resource \"hcloud_server\" \"mainServer\" { name = \"main-server\" image = \"debian-13\" server_type = \"cx23\" firewall_ids = [hcloud_firewall.sshFw.id] ssh_keys = [hcloud_ssh_key.loginUser1.id, hcloud_ssh_key.loginUser2.id] } 3. Return important information about the server To display important information in the console, it is recommended to create a separate file called output.tf and add the required content in different blocks. With the following example, the IP-adress will be returned after creating the server. output \"instance_public_ip\" { description = \"ip_adress\" value = hcloud_server.mainServer.ipv4_address } 4. Git To enable collaborative work, create a Git project and add the team members\u2019 SSH keys to the repository hosting platform so that each user can access the project securely. Result files","title":"Exercise 13"},{"location":"Exercises/Ex13/#exercise-13","text":"","title":"Exercise 13"},{"location":"Exercises/Ex13/#goal","text":"The Goal is to automate the creation of a server provided bei Hetzner with terraform. To avoid possible security vulnerabilities originating from login with password, our created server should only be accessed via SSH-key login . Important information such as the IP-address, which is necessary to connect to the server, has to be returned in the console after creating it. Additionally, the server configuration must be tracked with git .","title":"Goal"},{"location":"Exercises/Ex13/#steps","text":"","title":"Steps"},{"location":"Exercises/Ex13/#1-automation","text":"The first step is to set up a main.tf file with minimal terraform configuration, including the Cloud Provider Hetzner, the API token and the initial creation of the server. Running this file with $ terraform apply will start up a Server provided by Hetzner. # Define Hetzner cloud provider terraform { required_providers { hcloud = { source = \"hetznercloud/hcloud\" } } required_version = \">= 0.13\" } # Configure the Hetzner Cloud API token provider \"hcloud\" { token = var.hcloud_token } # Create a server resource \"hcloud_server\" \"mainServer\" { name = \"main-server\" image = \"debian-13\" server_type = \"cx23\" } The variable \"token\" is defined in a different file called variables.tf. variable \"hcloud_token\" { nullable = false sensitive = true } TODO: explain how to get and set token as environment variable","title":"1. Automation"},{"location":"Exercises/Ex13/#2-login-via-ssh-key","text":"To ensure that the created server can only be accessed via SSH key authentication, the main.tf file has to contain a firewall configuration which only allows port 22 (SSH). resource \"hcloud_firewall\" \"sshFw\" { name = \"ssh-firewall\" rule { direction = \"in\" protocol = \"tcp\" port = \"22\" source_ips = [\"0.0.0.0/0\", \"::/0\"] } } Following the setup of the firewall, the public keys of the authorized users must be added to main.tf. The public/private key pairs were previously created with $ ssh-keygen -a 256 -t ed25519 . #SSH-Keys resource \"hcloud_ssh_key\" \"loginUser1\" { name = \"se109@hdm-stuttgart.de\" public_key = file(\"../ssh/id_ed25519.pub\") } resource \"hcloud_ssh_key\" \"loginUser2\" { name = \"jb321@hdm-stuttgart.de\" public_key = file(\"../ssh/id_ed25519_infra.pub\") } The block where the server is created has to be modified to contain the SSH-keys. # Create a server resource \"hcloud_server\" \"mainServer\" { name = \"main-server\" image = \"debian-13\" server_type = \"cx23\" firewall_ids = [hcloud_firewall.sshFw.id] ssh_keys = [hcloud_ssh_key.loginUser1.id, hcloud_ssh_key.loginUser2.id] }","title":"2. Login via SSH-key"},{"location":"Exercises/Ex13/#3-return-important-information-about-the-server","text":"To display important information in the console, it is recommended to create a separate file called output.tf and add the required content in different blocks. With the following example, the IP-adress will be returned after creating the server. output \"instance_public_ip\" { description = \"ip_adress\" value = hcloud_server.mainServer.ipv4_address }","title":"3. Return important information about the server"},{"location":"Exercises/Ex13/#4-git","text":"To enable collaborative work, create a Git project and add the team members\u2019 SSH keys to the repository hosting platform so that each user can access the project securely.","title":"4. Git"},{"location":"Exercises/Ex13/#result","text":"files","title":"Result"},{"location":"Exercises/Ex14/","text":"Exercise 14 Goal By the end of this exercise the created server installs the latest version of nginx automatically and also starts nginx as a service. Nginx has to be always running so it also makes sure it restarts the service a rebooting. Steps 1. Create bash file (init.sh) Firstly the Script file for bash has to created. To make the file recognizeable as a bash script it has to start with this line: #!/bin/sh 2. Automatic system update and upgrade Since the installed linux system on new system is outdated most of the time, the first action should always be to update the repositories and installing the newest versions of all packages. For this two commands are needed: \"apt update\" checks the list of available packages and versions while \"apt upgrade\" installs the latest versions of the installed packages. Add commands to init.sh. aptitude update && aptitude upgrade -y The \"&&\" operator executes following commands if the previous command executed successful. The \"-y\"-flag makes it possible to execute the command without any interactive input of an user since it just confirms any questions with yes. 3. Nginx installation New Packages can be installed with the \"apt\"-package manager. Command for nginx installation has to be added to script. apt install nginx -y 4. Enable nginx The \"systemctl\" command is used to start, stop, check on or enable services. Enabling services with systemctl ensures that the services is always running even after a restart of the system. So the service always start after a boot. Following commands enable and start the nginx service. systemctl enable nginx systemctl start nginx 5. Integrate init.sh into terraform script Add init.sh script to server by setting the \"user_data\"-variable of the configuration of the server to a file(\"{path}\"), where the path points to the script file. # Create a server resource \"hcloud_server\" \"mainServer\" { name = \"main-server\" image = \"debian-13\" server_type = \"cx23\" user_data = file(\"init.sh\") # ! firewall_ids = [hcloud_firewall.sshFw.id] ssh_keys = [hcloud_ssh_key.loginUser1.id, hcloud_ssh_key.loginUser2.id] } Result files","title":"Exercise 14"},{"location":"Exercises/Ex14/#exercise-14","text":"","title":"Exercise 14"},{"location":"Exercises/Ex14/#goal","text":"By the end of this exercise the created server installs the latest version of nginx automatically and also starts nginx as a service. Nginx has to be always running so it also makes sure it restarts the service a rebooting.","title":"Goal"},{"location":"Exercises/Ex14/#steps","text":"","title":"Steps"},{"location":"Exercises/Ex14/#1-create-bash-file-initsh","text":"Firstly the Script file for bash has to created. To make the file recognizeable as a bash script it has to start with this line: #!/bin/sh","title":"1. Create bash file (init.sh)"},{"location":"Exercises/Ex14/#2-automatic-system-update-and-upgrade","text":"Since the installed linux system on new system is outdated most of the time, the first action should always be to update the repositories and installing the newest versions of all packages. For this two commands are needed: \"apt update\" checks the list of available packages and versions while \"apt upgrade\" installs the latest versions of the installed packages. Add commands to init.sh. aptitude update && aptitude upgrade -y The \"&&\" operator executes following commands if the previous command executed successful. The \"-y\"-flag makes it possible to execute the command without any interactive input of an user since it just confirms any questions with yes.","title":"2. Automatic system update and upgrade"},{"location":"Exercises/Ex14/#3-nginx-installation","text":"New Packages can be installed with the \"apt\"-package manager. Command for nginx installation has to be added to script. apt install nginx -y","title":"3. Nginx installation"},{"location":"Exercises/Ex14/#4-enable-nginx","text":"The \"systemctl\" command is used to start, stop, check on or enable services. Enabling services with systemctl ensures that the services is always running even after a restart of the system. So the service always start after a boot. Following commands enable and start the nginx service. systemctl enable nginx systemctl start nginx","title":"4. Enable nginx"},{"location":"Exercises/Ex14/#5-integrate-initsh-into-terraform-script","text":"Add init.sh script to server by setting the \"user_data\"-variable of the configuration of the server to a file(\"{path}\"), where the path points to the script file. # Create a server resource \"hcloud_server\" \"mainServer\" { name = \"main-server\" image = \"debian-13\" server_type = \"cx23\" user_data = file(\"init.sh\") # ! firewall_ids = [hcloud_firewall.sshFw.id] ssh_keys = [hcloud_ssh_key.loginUser1.id, hcloud_ssh_key.loginUser2.id] }","title":"5. Integrate init.sh into terraform script"},{"location":"Exercises/Ex14/#result","text":"files","title":"Result"},{"location":"Exercises/Ex15/","text":"Exercise 15 Goal To further automate the server's configuration and setup, it must be initialized by using cloud-init instead of the previously written Bash-script. It is important to ensure that our server is as secure as possible against unauthorized access. This is accomplished by disabling root login and only allowing one (sudo enabled) \"devops\" user to login via SSH-Key . The server should be upgraded on creation and rebooted if required, to ensure that all packages are up to date. In addition, the packages plocat and fail2ban must be installed and configured. Steps 1. Cloud-init First, create a cloud-init template and add the required packages to it: packages: - nginx - btop Make sure that the packages are always up-to-date: package_update: true package_upgrade: true package_reboot_if_required: true It can be helpful for upcoming exercises to create a separate folder where template files are stored. The structure can look like this for example: HIER BILD EINF\u00dcGEN Add the custom command runmcd to the template, to make sure the Nginx server is set up correctly with initialization. runcmd: - systemctl enable nginx - rm /var/www/html/* - > echo \"I'm Nginx @ $(dig -4 TXT +short o-o.myaddr.l.google.com @ns1.google.com) created $(date -u)\" >> /var/www/html/index.html Afterwards, set the private SSH-keys in cloud-init template so that they are added to authorized_keys on server: # ssh configuration ${yamlencode( { ssh_keys = { ed25519_private = host_ed25519_private ed25519_public = host_ed25519_public } } )} This section should be written in JSON format instead of YAML. This ensures correct processing by using yamlencode and avoids formatting issues. While the indent function can be used as a workaround, it is not an elegant solution. Make sure to integrate it in the main.tf as well by setting the SSH-configuration of the server to the authorized keys: resource \"local_file\" \"user_data\" { content = templatefile(\"template/cloud-template.yml\", { host_ed25519_private = tls_private_key.host.private_key_openssh host_ed25519_public = tls_private_key.host.public_key_openssh }) filename = \"../gen/cloud-template.yml\" } Then add the following line of code to the # Create a server block in main.tf: user_data = local_file.user_data.content Additionally, the main.tf has to contain the following block: resource \"local_file\" \"known_hosts\" { content = join(\" \" ,[ hcloud_server.mainServer.ipv4_address , tls_private_key.host.public_key_openssh ] ) filename = \"../gen/known_hosts\" file_permission = \"644\" } Upon creating the server, a new file cloud-template.yml , which contains the public and private key of the authorized user, will be generated and stored in a new folder like this: HIER BILD EINF\u00dcGEN!! We intend to disable root login while ensuring that we can still access the server via SSH. To achieve this, the first step is to create a \"devops\" user in our cloud-template.yml and bind our public keys to this user instead of root: # user configuration users: - name: ${devopsUsername} ssh_authorized_keys: - ${public_ssh_user1} - ${public_ssh_user2} shell: /bin/bash Then integrate it into the main.tf as well by adding the authorized keys to the previously added block: resource \"local_file\" \"user_data\" { content = templatefile(\"template/cloud-template.yml\", { host_ed25519_private = tls_private_key.host.private_key_openssh host_ed25519_public = tls_private_key.host.public_key_openssh devopsUsername = \"devops\" public_ssh_user1 = hcloud_ssh_key.loginUser1.public_key # ! public_ssh_user2 = hcloud_ssh_key.loginUser2.public_key # ! }) filename = \"../gen/cloud-template.yml\" } Now, we have to disable SSH-password login for everyone by adding the line ssh_pwauth: False to the cloud-template. To ensure that the devops user has root privileges and can execute sudo commands, add the following line to the user configuration in the cloud-template: sudo: ['ALL=(ALL) NOPASSWD:ALL'] Then disable root login alltogether by adding the line passwd -l root to the runcmd block. add fail2ban and plocate to cloud-init and also add updatedb command to runcmd-section TODO: add fail2ban configuration (copy this into jail.local: [sshd] backend=systemd enabled = true port = ssh filter = sshd logpath = /var/log/auth.log maxretry = 3) Problems (3)when using yaml for ssh keys the formatting is corrupt and ssh-server cant read private key correctly. Result files","title":"Exercise 15"},{"location":"Exercises/Ex15/#exercise-15","text":"","title":"Exercise 15"},{"location":"Exercises/Ex15/#goal","text":"To further automate the server's configuration and setup, it must be initialized by using cloud-init instead of the previously written Bash-script. It is important to ensure that our server is as secure as possible against unauthorized access. This is accomplished by disabling root login and only allowing one (sudo enabled) \"devops\" user to login via SSH-Key . The server should be upgraded on creation and rebooted if required, to ensure that all packages are up to date. In addition, the packages plocat and fail2ban must be installed and configured.","title":"Goal"},{"location":"Exercises/Ex15/#steps","text":"","title":"Steps"},{"location":"Exercises/Ex15/#1-cloud-init","text":"First, create a cloud-init template and add the required packages to it: packages: - nginx - btop Make sure that the packages are always up-to-date: package_update: true package_upgrade: true package_reboot_if_required: true It can be helpful for upcoming exercises to create a separate folder where template files are stored. The structure can look like this for example: HIER BILD EINF\u00dcGEN Add the custom command runmcd to the template, to make sure the Nginx server is set up correctly with initialization. runcmd: - systemctl enable nginx - rm /var/www/html/* - > echo \"I'm Nginx @ $(dig -4 TXT +short o-o.myaddr.l.google.com @ns1.google.com) created $(date -u)\" >> /var/www/html/index.html Afterwards, set the private SSH-keys in cloud-init template so that they are added to authorized_keys on server: # ssh configuration ${yamlencode( { ssh_keys = { ed25519_private = host_ed25519_private ed25519_public = host_ed25519_public } } )} This section should be written in JSON format instead of YAML. This ensures correct processing by using yamlencode and avoids formatting issues. While the indent function can be used as a workaround, it is not an elegant solution. Make sure to integrate it in the main.tf as well by setting the SSH-configuration of the server to the authorized keys: resource \"local_file\" \"user_data\" { content = templatefile(\"template/cloud-template.yml\", { host_ed25519_private = tls_private_key.host.private_key_openssh host_ed25519_public = tls_private_key.host.public_key_openssh }) filename = \"../gen/cloud-template.yml\" } Then add the following line of code to the # Create a server block in main.tf: user_data = local_file.user_data.content Additionally, the main.tf has to contain the following block: resource \"local_file\" \"known_hosts\" { content = join(\" \" ,[ hcloud_server.mainServer.ipv4_address , tls_private_key.host.public_key_openssh ] ) filename = \"../gen/known_hosts\" file_permission = \"644\" } Upon creating the server, a new file cloud-template.yml , which contains the public and private key of the authorized user, will be generated and stored in a new folder like this: HIER BILD EINF\u00dcGEN!! We intend to disable root login while ensuring that we can still access the server via SSH. To achieve this, the first step is to create a \"devops\" user in our cloud-template.yml and bind our public keys to this user instead of root: # user configuration users: - name: ${devopsUsername} ssh_authorized_keys: - ${public_ssh_user1} - ${public_ssh_user2} shell: /bin/bash Then integrate it into the main.tf as well by adding the authorized keys to the previously added block: resource \"local_file\" \"user_data\" { content = templatefile(\"template/cloud-template.yml\", { host_ed25519_private = tls_private_key.host.private_key_openssh host_ed25519_public = tls_private_key.host.public_key_openssh devopsUsername = \"devops\" public_ssh_user1 = hcloud_ssh_key.loginUser1.public_key # ! public_ssh_user2 = hcloud_ssh_key.loginUser2.public_key # ! }) filename = \"../gen/cloud-template.yml\" } Now, we have to disable SSH-password login for everyone by adding the line ssh_pwauth: False to the cloud-template. To ensure that the devops user has root privileges and can execute sudo commands, add the following line to the user configuration in the cloud-template: sudo: ['ALL=(ALL) NOPASSWD:ALL'] Then disable root login alltogether by adding the line passwd -l root to the runcmd block. add fail2ban and plocate to cloud-init and also add updatedb command to runcmd-section TODO: add fail2ban configuration (copy this into jail.local: [sshd] backend=systemd enabled = true port = ssh filter = sshd logpath = /var/log/auth.log maxretry = 3)","title":"1. Cloud-init"},{"location":"Exercises/Ex15/#problems","text":"(3)when using yaml for ssh keys the formatting is corrupt and ssh-server cant read private key correctly.","title":"Problems"},{"location":"Exercises/Ex15/#result","text":"files","title":"Result"},{"location":"Exercises/Ex16/","text":"Exercise 16 Goal Since the public and private ssh key of the server is created by the terraform script it is possible to add the pubilc key to a \"known_hosts\" file. Consequently ssh does not have to accept any unkown public key fingerprints and ensures we are connected to the right server. This makes the ssh connection a bit more secure but also little inconvenient because the \"known_hosts\" file has to be manually linked when executing the ssh command. To fix this terraform can create a bash script with the ssh command to the correct \"known_hosts\" file, username and host. Because this more practical and secure it should also create a bash script to copy files to the server with \"scp\". Steps 1. Create bash file in template directory Has to begin with: #!/usr/bin/env bash 2. Write ssh command that uses the generated known_host file GEN_DIR=$(dirname \"$0\")/../gen ssh -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" user@host \"$@\" The variable GEN_DIR always points to the directory gen where the known_hosts file is located since the scripts are always in the bin directory that is located right next to the gen directory The option -o UserKnownHostsFile is used to set a custom known_hosts for the ssh connection $@ appends any option that is set when executing the script to the ssh command 3. Add placeholder variables for the terraform template ssh (...) ${devopsUsername}@${ip} The variable devopsUsername and ip can be set by terraform when generating the script based of the template 4. Let terraform generated the script based of the template resource \"local_file\" \"ssh_script\" { content = templatefile(\"{path.module}/template/ssh-template.sh\", { devopsUsername = var.devopsUsername, ip = hcloud_server.mainServer.ipv4_address }) filename = \"../bin/ssh.sh\" } The right resource type for this is local_file which offers two important attributes: content contains the text that should be written in the file. filename contains the path to the directory where the file should be saved aswell as the filename with extension. The templatefile() function reads the content of the template file and replaces the given keywords ( devopsUsername and ip ) with the assosiated values ( var.devopsUsername and hcloud_server.mainServer.ipv4_address ) ### 4. Execute the generated script with bash - The generated file may look like this and can be executed in the terminal like this: bash ./ssh.sh #!/usr/bin/env bash GEN_DIR=$(dirname \"$0\")/../gen ssh -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" devops@77.42.37.25 \"$@\" 5. Repeat everything with scp #!/usr/bin/env bash GEN_DIR=$(dirname \"$0\")/../gen if [ $# -lt 2 ]; then echo usage: .../bin/scp ... devops@157.180.78.16 ... else scp -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" $1 ${devopsUsername}@${ip}:$2 fi The scp script is structured very similar, but since it requires additional user arguments for the path of the source and destination the script checks that at least two arguments are given This would be an example usage of the scp script that copies a ssh public key to the server bash ./scp.sh ~/.ssh/id_ed25519.pub ~/.ssh/id_ed25519.pub Result files","title":"Exercise 16"},{"location":"Exercises/Ex16/#exercise-16","text":"","title":"Exercise 16"},{"location":"Exercises/Ex16/#goal","text":"Since the public and private ssh key of the server is created by the terraform script it is possible to add the pubilc key to a \"known_hosts\" file. Consequently ssh does not have to accept any unkown public key fingerprints and ensures we are connected to the right server. This makes the ssh connection a bit more secure but also little inconvenient because the \"known_hosts\" file has to be manually linked when executing the ssh command. To fix this terraform can create a bash script with the ssh command to the correct \"known_hosts\" file, username and host. Because this more practical and secure it should also create a bash script to copy files to the server with \"scp\".","title":"Goal"},{"location":"Exercises/Ex16/#steps","text":"","title":"Steps"},{"location":"Exercises/Ex16/#1-create-bash-file-in-template-directory","text":"Has to begin with: #!/usr/bin/env bash","title":"1. Create bash file in template directory"},{"location":"Exercises/Ex16/#2-write-ssh-command-that-uses-the-generated-known_host-file","text":"GEN_DIR=$(dirname \"$0\")/../gen ssh -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" user@host \"$@\" The variable GEN_DIR always points to the directory gen where the known_hosts file is located since the scripts are always in the bin directory that is located right next to the gen directory The option -o UserKnownHostsFile is used to set a custom known_hosts for the ssh connection $@ appends any option that is set when executing the script to the ssh command","title":"2. Write ssh command that uses the generated known_host file"},{"location":"Exercises/Ex16/#3-add-placeholder-variables-for-the-terraform-template","text":"ssh (...) ${devopsUsername}@${ip} The variable devopsUsername and ip can be set by terraform when generating the script based of the template","title":"3. Add placeholder variables for the terraform template"},{"location":"Exercises/Ex16/#4-let-terraform-generated-the-script-based-of-the-template","text":"resource \"local_file\" \"ssh_script\" { content = templatefile(\"{path.module}/template/ssh-template.sh\", { devopsUsername = var.devopsUsername, ip = hcloud_server.mainServer.ipv4_address }) filename = \"../bin/ssh.sh\" } The right resource type for this is local_file which offers two important attributes: content contains the text that should be written in the file. filename contains the path to the directory where the file should be saved aswell as the filename with extension. The templatefile() function reads the content of the template file and replaces the given keywords ( devopsUsername and ip ) with the assosiated values ( var.devopsUsername and hcloud_server.mainServer.ipv4_address ) ### 4. Execute the generated script with bash - The generated file may look like this and can be executed in the terminal like this: bash ./ssh.sh #!/usr/bin/env bash GEN_DIR=$(dirname \"$0\")/../gen ssh -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" devops@77.42.37.25 \"$@\"","title":"4. Let terraform generated the script based of the template"},{"location":"Exercises/Ex16/#5-repeat-everything-with-scp","text":"#!/usr/bin/env bash GEN_DIR=$(dirname \"$0\")/../gen if [ $# -lt 2 ]; then echo usage: .../bin/scp ... devops@157.180.78.16 ... else scp -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" $1 ${devopsUsername}@${ip}:$2 fi The scp script is structured very similar, but since it requires additional user arguments for the path of the source and destination the script checks that at least two arguments are given This would be an example usage of the scp script that copies a ssh public key to the server bash ./scp.sh ~/.ssh/id_ed25519.pub ~/.ssh/id_ed25519.pub","title":"5. Repeat everything with scp"},{"location":"Exercises/Ex16/#result","text":"files","title":"Result"},{"location":"Exercises/Ex17/","text":"Exercise 17 Goal By the end of this exercise, a Terraform module generates a JSON file containing metadata of a created server. The module takes several input variables (such as host name, IP addresses, location, and volume information) and renders them into a JSON file using a template. This JSON file can later be used by other tools or scripts for documentation or automation purposes. Steps 1. Create a folder for the module and the necesarry files The module structure should look like this: HostMetaData \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 outputs.tf \u251c\u2500\u2500 tpl \u2502 \u2514\u2500\u2500 hostdata.json \u2514\u2500\u2500 variables.tf variables.tf - holds all the variables/hostdata for the module to be used when creating the json hostdata.json - contains a json template with placeholder for all the variables main.tf - generates the json file by taking the hostdata from the variables.tf and replacing the placeholder in the json template outputs.tf - outputs all the hostdata to the console for extended useabilty 2. Variables used by the module The module expects several string input variables definded in the variables.tf : name \u2013 Name of the host ipv4_address \u2013 public IPv4 address of the host ipv6_address \u2013 public IPv6 address of the host location \u2013 Physical or logical location of the host ( volume_linuxdevice \u2013 optional Linux device name of the attached volume) These variables are directly mapped into the JSON output via the main.tf file. 3. Generate the json content The template file is a valid json file with placeholder variables for the real data. { \"name\": \"${name}\", \"network\": { \"ipv4\": \"${ip4}\", \"ipv6\": \"${ip6}\" }, \"location\": \"${location}\", \"volume\": \"${volume}\" } Terraform\u2019s templatefile() function is used to generate the json and mapping the modules variables to the placeholder in the template. The template file is located in the tpl/ directory. templatefile( \"${path.module}/tpl/hostdata.json\", { name = var.name ip4 = var.ipv4_address ip6 = var.ipv6_address location = var.location volume = var.volume_linuxdevice } ) ${path.module} ensures the path is resolved relative to the module directory. 4. Save the json content in a local file After this Terraform can save the json in a file by creating a local_file resource and setting the content to the generated json. resource \"local_file\" \"hostdata\" { content = templatefile(...) filename = \"gen/${var.name}.json\" } The filename defindes where the file is saved. In this case in the gen directory relativly to the location where the module is used. Also the name of the file is dynamically set based on the name of the host. Each host therefore gets its own JSON file. The gen/ directory is used to store all generated metadata files in one place. Result files","title":"Exercise 17"},{"location":"Exercises/Ex17/#exercise-17","text":"","title":"Exercise 17"},{"location":"Exercises/Ex17/#goal","text":"By the end of this exercise, a Terraform module generates a JSON file containing metadata of a created server. The module takes several input variables (such as host name, IP addresses, location, and volume information) and renders them into a JSON file using a template. This JSON file can later be used by other tools or scripts for documentation or automation purposes.","title":"Goal"},{"location":"Exercises/Ex17/#steps","text":"","title":"Steps"},{"location":"Exercises/Ex17/#1-create-a-folder-for-the-module-and-the-necesarry-files","text":"The module structure should look like this: HostMetaData \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 outputs.tf \u251c\u2500\u2500 tpl \u2502 \u2514\u2500\u2500 hostdata.json \u2514\u2500\u2500 variables.tf variables.tf - holds all the variables/hostdata for the module to be used when creating the json hostdata.json - contains a json template with placeholder for all the variables main.tf - generates the json file by taking the hostdata from the variables.tf and replacing the placeholder in the json template outputs.tf - outputs all the hostdata to the console for extended useabilty","title":"1. Create a folder for the module and the necesarry files"},{"location":"Exercises/Ex17/#2-variables-used-by-the-module","text":"The module expects several string input variables definded in the variables.tf : name \u2013 Name of the host ipv4_address \u2013 public IPv4 address of the host ipv6_address \u2013 public IPv6 address of the host location \u2013 Physical or logical location of the host ( volume_linuxdevice \u2013 optional Linux device name of the attached volume) These variables are directly mapped into the JSON output via the main.tf file.","title":"2. Variables used by the module"},{"location":"Exercises/Ex17/#3-generate-the-json-content","text":"The template file is a valid json file with placeholder variables for the real data. { \"name\": \"${name}\", \"network\": { \"ipv4\": \"${ip4}\", \"ipv6\": \"${ip6}\" }, \"location\": \"${location}\", \"volume\": \"${volume}\" } Terraform\u2019s templatefile() function is used to generate the json and mapping the modules variables to the placeholder in the template. The template file is located in the tpl/ directory. templatefile( \"${path.module}/tpl/hostdata.json\", { name = var.name ip4 = var.ipv4_address ip6 = var.ipv6_address location = var.location volume = var.volume_linuxdevice } ) ${path.module} ensures the path is resolved relative to the module directory.","title":"3. Generate the json content"},{"location":"Exercises/Ex17/#4-save-the-json-content-in-a-local-file","text":"After this Terraform can save the json in a file by creating a local_file resource and setting the content to the generated json. resource \"local_file\" \"hostdata\" { content = templatefile(...) filename = \"gen/${var.name}.json\" } The filename defindes where the file is saved. In this case in the gen directory relativly to the location where the module is used. Also the name of the file is dynamically set based on the name of the host. Each host therefore gets its own JSON file. The gen/ directory is used to store all generated metadata files in one place.","title":"4. Save the json content in a local file"},{"location":"Exercises/Ex17/#result","text":"files","title":"Result"},{"location":"Exercises/Ex18/","text":"Exercise 18 Goal In exercise 16 we setup a terraform configuration that automatically generates a known_hosts file and a ssh/scp script for convenient and secure connection to the server. In this exercise this functionality is refactored to a seperate module to improve reuseability. Steps 1. Create module directory Firstly a main.tf as entrypoint for the module is created. In addition, there is also a variables.tf for all the data where the module depends on the specific host metadata. The strucuture should look like this: SshKnownHosts \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 tpl \u2502 \u251c\u2500\u2500 scp-template.sh \u2502 \u2514\u2500\u2500 ssh-template.sh \u2514\u2500\u2500 variables.tf The templates for the ssh/scp scripts are 2. Module usage The module is instantiated by passing user and server-specific information. The IPv4 address and SSH host key are usually retrieved from Terraform-managed resources. module \"createSshKnownHosts\" { source = \"../modules/SshKnownHosts\" loginUsername = var.devopsUsername serverNameOrIp = hcloud_server.mainServer.ipv4_address serverHostPublicKey = tls_private_key.host.public_key_openssh } loginUsername defines the SSH user serverNameOrIp defines the SSH target serverHostPublicKey ensures host authenticity 3. Creating the custom known_hosts file The local_file resource is used to create a project-local known_hosts file. Host address and public key are joined into the OpenSSH format. resource \"local_file\" \"known_hosts\" { content = join(\" \", [ var.serverNameOrIp, var.serverHostPublicKey ]) filename = \"gen/known_hosts\" file_permission = \"644\" } The file is written to the gen/ directory File permissions allow reading without requiring elevated privileges 4. Generating the SSH wrapper script A template file is used to generate a reusable SSH script. The script forces SSH to use the generated known_hosts file. resource \"local_file\" \"ssh_script\" { content = templatefile(\"${path.module}/tpl/ssh-template.yml\", { username = var.loginUsername ip = var.serverNameOrIp }) filename = \"bin/ssh.sh\" } ssh-template.yml #!/usr/bin/env bash GEN_DIR=$(dirname \"$0\")/../gen ssh -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" ${username}@${ip} \"$@\" $@ forwards all arguments to the SSH command The script behaves exactly like ssh, but with a fixed known_hosts file 5. Generating the SCP wrapper script A second template creates a secure SCP helper. The script validates arguments and enforces the custom known_hosts file. resource \"local_file\" \"scp_script\" { content = templatefile(\"${path.module}/tpl/scp-template.yml\", { username = var.loginUsername ip = var.serverNameOrIp }) filename = \"bin/scp.sh\" } scp-template.yml #!/usr/bin/env bash GEN_DIR=$(dirname \"$0\")/../gen if [ $# -lt 2 ]; then echo usage: .../bin/scp ... devops@157.180.78.16 ... else scp -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" $1 ${username}@${ip}:$2 fi Ensures at least source and destination are provided Prevents accidental misuse of the script 6. Variables used by the module The module expects the following input variables: loginUsername \u2013 SSH login user serverNameOrIp \u2013 Hostname or IP address of the server serverHostPublicKey \u2013 Public SSH host key These values are injected into both the known_hosts file and scripts. Result files","title":"Exercise 18"},{"location":"Exercises/Ex18/#exercise-18","text":"","title":"Exercise 18"},{"location":"Exercises/Ex18/#goal","text":"In exercise 16 we setup a terraform configuration that automatically generates a known_hosts file and a ssh/scp script for convenient and secure connection to the server. In this exercise this functionality is refactored to a seperate module to improve reuseability.","title":"Goal"},{"location":"Exercises/Ex18/#steps","text":"","title":"Steps"},{"location":"Exercises/Ex18/#1-create-module-directory","text":"Firstly a main.tf as entrypoint for the module is created. In addition, there is also a variables.tf for all the data where the module depends on the specific host metadata. The strucuture should look like this: SshKnownHosts \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 tpl \u2502 \u251c\u2500\u2500 scp-template.sh \u2502 \u2514\u2500\u2500 ssh-template.sh \u2514\u2500\u2500 variables.tf The templates for the ssh/scp scripts are","title":"1. Create module directory"},{"location":"Exercises/Ex18/#2-module-usage","text":"The module is instantiated by passing user and server-specific information. The IPv4 address and SSH host key are usually retrieved from Terraform-managed resources. module \"createSshKnownHosts\" { source = \"../modules/SshKnownHosts\" loginUsername = var.devopsUsername serverNameOrIp = hcloud_server.mainServer.ipv4_address serverHostPublicKey = tls_private_key.host.public_key_openssh } loginUsername defines the SSH user serverNameOrIp defines the SSH target serverHostPublicKey ensures host authenticity","title":"2. Module usage"},{"location":"Exercises/Ex18/#3-creating-the-custom-known_hosts-file","text":"The local_file resource is used to create a project-local known_hosts file. Host address and public key are joined into the OpenSSH format. resource \"local_file\" \"known_hosts\" { content = join(\" \", [ var.serverNameOrIp, var.serverHostPublicKey ]) filename = \"gen/known_hosts\" file_permission = \"644\" } The file is written to the gen/ directory File permissions allow reading without requiring elevated privileges","title":"3. Creating the custom known_hosts file"},{"location":"Exercises/Ex18/#4-generating-the-ssh-wrapper-script","text":"A template file is used to generate a reusable SSH script. The script forces SSH to use the generated known_hosts file. resource \"local_file\" \"ssh_script\" { content = templatefile(\"${path.module}/tpl/ssh-template.yml\", { username = var.loginUsername ip = var.serverNameOrIp }) filename = \"bin/ssh.sh\" } ssh-template.yml #!/usr/bin/env bash GEN_DIR=$(dirname \"$0\")/../gen ssh -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" ${username}@${ip} \"$@\" $@ forwards all arguments to the SSH command The script behaves exactly like ssh, but with a fixed known_hosts file","title":"4. Generating the SSH wrapper script"},{"location":"Exercises/Ex18/#5-generating-the-scp-wrapper-script","text":"A second template creates a secure SCP helper. The script validates arguments and enforces the custom known_hosts file. resource \"local_file\" \"scp_script\" { content = templatefile(\"${path.module}/tpl/scp-template.yml\", { username = var.loginUsername ip = var.serverNameOrIp }) filename = \"bin/scp.sh\" } scp-template.yml #!/usr/bin/env bash GEN_DIR=$(dirname \"$0\")/../gen if [ $# -lt 2 ]; then echo usage: .../bin/scp ... devops@157.180.78.16 ... else scp -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" $1 ${username}@${ip}:$2 fi Ensures at least source and destination are provided Prevents accidental misuse of the script","title":"5. Generating the SCP wrapper script"},{"location":"Exercises/Ex18/#6-variables-used-by-the-module","text":"The module expects the following input variables: loginUsername \u2013 SSH login user serverNameOrIp \u2013 Hostname or IP address of the server serverHostPublicKey \u2013 Public SSH host key These values are injected into both the known_hosts file and scripts.","title":"6. Variables used by the module"},{"location":"Exercises/Ex18/#result","text":"files","title":"Result"},{"location":"Exercises/Ex19/","text":"Exercise 19 Goal This exercise is meant to teach us about partitions and mount procedures by adding a volume to the server, which is automounted at first. To get familiar with mounting and how it works, we have to create two partitions manually after starting up the server and observe what happens when we unmount them. Afterwards, we create specific mount points and mount the partitions manually to them. To ensure the mounting becomes permenant , it is required to happen automatically upon server reboot. Steps 1. Volume creation Extend the main.tf by adding a new rescource block, containing configuration for a volume with the size of 10 GB. Enable automounting by setting the parameter automount to true and connect the volume to the server by using server_id. Without this server connection, the volume cannot be attached correctly, which makes this step essential for automounting. resource \"hcloud_volume\" \"volume01\" { name = \"volume1\" size = 10 server_id = hcloud_server.mainServer.id # ! automount = true # ! format = \"xfs\" } Now include an output directive to the output.tf file, returning information about the linux device of our volume. output \"instance_volumeLinuxDevice\" { description = \"Linux device of volume\" value = hcloud_volume.volume01.linux_device } To check if it worked, start up the server, login and try the command df . HIER BILD EINF\u00dcGEN VON NOTIZEN Note: In this documentary, the specific volume-ID HC_Volume_104253772 is used. When recreating the steps, the number can vary, depending on which specific volume in the storage pool the data is being read from or written to. A known issue of automounting is that it does not work corrcetly if the server is rebooted, preventing the existing volume from being mounted to the recreated instance. This problem orinates from the run cmd command in the cloud-template.yml and can be circumvented by adding the following line: udevadm trigger -c add -s block -p ID_VENDOR=HC --verbose -p ID_MODEL=Volume . This command would be executed by default, if it would not be overwritten by our own defined runmcd. The volume is now available after rebooting. Source: https://github.com/hetznercloud/terraform-provider-hcloud/issues/473#issuecomment-971535629-permalink 2. Unmounting After starting up the server and making sure the volume is set up corrcetly, use cd /mnt/HC_Volume_104253772 to get to the diractionary where it is mounted. When trying to unmount the volume by using sudo umount , an error pops up, telling us that the file is busy. Work around that issue by going to a different diractionary and use sudo umount /mnt/HC_Volume_104253772 again. To test if the unmounting got executed correctly, we put some test.txt files into the folder. After unmounting successfully, the files disappear. They reapear after mounting the volume again with the command /dev/sdb/ /mnt/HC_Volume_104253772 . 3. Creating partitions The command for creating partitions manually is fdisk /dev/sdb/ . Create a new partition interactively: Press n to add a new partition Choose p to set the partition type to primary Assign partition number 1 first sector is 2048 by default set the last sector to half if the available volume space. Because the volume should be split into two partitions of approximately equal size (\u22485 GB each), the end sector of the first partition is calculated as: (last usable sector \u2212 first sector) / 2 + first sector In our case the first partition spans from sector 2048 to 10484735. Press w to safe and exit. To create the second partition, the same procedure is followed. The second partition automatically starts at the next available sector and spans the remaining disk space, completing the split into two approximately equal partitions. To change formating of the partitions, it is required to unmount the volume again. Execute the command mkfs -t ext4 /dev/sdb1 for creating an ext4 file system on your first partition. Then use mkfs -t xfs /dev/sdb2 for creating an xfs file system on your second partition. 4. Manual mounting To manually mount the previously created partitions, create two mountpoints \"disk1\" and disk2\" by executing mkdir /disk1 /disk2 Mount the first partition manually using the device name: mount /dev/sdb1 /disk1 . The second partition should be mounted by using its UUID. To get the UUID value, use blkid /dev/sdb2 . Mount the second partition with the following command: mount UUID=<uuid-of-sdb2> /disk2 . 5. Persistent mounting To ensure the mounts stay persistent upon rebooting, edit the /etc/fstab file by using the nano command. Add the following content: /dev/sdb1 /disk1 ext4 discard,nofail,defaults 0 2 and UUID=<uuid-of-sdb2> /disk2 xfs discard,nofail,defaults 0 2 Instead of UUIDs, it is possible to assign labels to the partitions and use them for mounting. The labels are assigned by using e2label /dev/sdb1 part1 . Do the same with sdb2 and label it with part2. The /etc/fstab file now must contain the following lines: LABEL=part1 /disk1 ext4 discard,nofail,defaults 0 2 and LABEL=part2 /disk2 xfs discard,nofail,defaults 0 2 . lik zum repository noch!!","title":"Exercise 19"},{"location":"Exercises/Ex19/#exercise-19","text":"","title":"Exercise 19"},{"location":"Exercises/Ex19/#goal","text":"This exercise is meant to teach us about partitions and mount procedures by adding a volume to the server, which is automounted at first. To get familiar with mounting and how it works, we have to create two partitions manually after starting up the server and observe what happens when we unmount them. Afterwards, we create specific mount points and mount the partitions manually to them. To ensure the mounting becomes permenant , it is required to happen automatically upon server reboot.","title":"Goal"},{"location":"Exercises/Ex19/#steps","text":"","title":"Steps"},{"location":"Exercises/Ex19/#1-volume-creation","text":"Extend the main.tf by adding a new rescource block, containing configuration for a volume with the size of 10 GB. Enable automounting by setting the parameter automount to true and connect the volume to the server by using server_id. Without this server connection, the volume cannot be attached correctly, which makes this step essential for automounting. resource \"hcloud_volume\" \"volume01\" { name = \"volume1\" size = 10 server_id = hcloud_server.mainServer.id # ! automount = true # ! format = \"xfs\" } Now include an output directive to the output.tf file, returning information about the linux device of our volume. output \"instance_volumeLinuxDevice\" { description = \"Linux device of volume\" value = hcloud_volume.volume01.linux_device } To check if it worked, start up the server, login and try the command df . HIER BILD EINF\u00dcGEN VON NOTIZEN Note: In this documentary, the specific volume-ID HC_Volume_104253772 is used. When recreating the steps, the number can vary, depending on which specific volume in the storage pool the data is being read from or written to. A known issue of automounting is that it does not work corrcetly if the server is rebooted, preventing the existing volume from being mounted to the recreated instance. This problem orinates from the run cmd command in the cloud-template.yml and can be circumvented by adding the following line: udevadm trigger -c add -s block -p ID_VENDOR=HC --verbose -p ID_MODEL=Volume . This command would be executed by default, if it would not be overwritten by our own defined runmcd. The volume is now available after rebooting. Source: https://github.com/hetznercloud/terraform-provider-hcloud/issues/473#issuecomment-971535629-permalink","title":"1. Volume creation"},{"location":"Exercises/Ex19/#2-unmounting","text":"After starting up the server and making sure the volume is set up corrcetly, use cd /mnt/HC_Volume_104253772 to get to the diractionary where it is mounted. When trying to unmount the volume by using sudo umount , an error pops up, telling us that the file is busy. Work around that issue by going to a different diractionary and use sudo umount /mnt/HC_Volume_104253772 again. To test if the unmounting got executed correctly, we put some test.txt files into the folder. After unmounting successfully, the files disappear. They reapear after mounting the volume again with the command /dev/sdb/ /mnt/HC_Volume_104253772 .","title":"2. Unmounting"},{"location":"Exercises/Ex19/#3-creating-partitions","text":"The command for creating partitions manually is fdisk /dev/sdb/ . Create a new partition interactively: Press n to add a new partition Choose p to set the partition type to primary Assign partition number 1 first sector is 2048 by default set the last sector to half if the available volume space. Because the volume should be split into two partitions of approximately equal size (\u22485 GB each), the end sector of the first partition is calculated as: (last usable sector \u2212 first sector) / 2 + first sector In our case the first partition spans from sector 2048 to 10484735. Press w to safe and exit. To create the second partition, the same procedure is followed. The second partition automatically starts at the next available sector and spans the remaining disk space, completing the split into two approximately equal partitions. To change formating of the partitions, it is required to unmount the volume again. Execute the command mkfs -t ext4 /dev/sdb1 for creating an ext4 file system on your first partition. Then use mkfs -t xfs /dev/sdb2 for creating an xfs file system on your second partition.","title":"3. Creating partitions"},{"location":"Exercises/Ex19/#4-manual-mounting","text":"To manually mount the previously created partitions, create two mountpoints \"disk1\" and disk2\" by executing mkdir /disk1 /disk2 Mount the first partition manually using the device name: mount /dev/sdb1 /disk1 . The second partition should be mounted by using its UUID. To get the UUID value, use blkid /dev/sdb2 . Mount the second partition with the following command: mount UUID=<uuid-of-sdb2> /disk2 .","title":"4. Manual mounting"},{"location":"Exercises/Ex19/#5-persistent-mounting","text":"To ensure the mounts stay persistent upon rebooting, edit the /etc/fstab file by using the nano command. Add the following content: /dev/sdb1 /disk1 ext4 discard,nofail,defaults 0 2 and UUID=<uuid-of-sdb2> /disk2 xfs discard,nofail,defaults 0 2 Instead of UUIDs, it is possible to assign labels to the partitions and use them for mounting. The labels are assigned by using e2label /dev/sdb1 part1 . Do the same with sdb2 and label it with part2. The /etc/fstab file now must contain the following lines: LABEL=part1 /disk1 ext4 discard,nofail,defaults 0 2 and LABEL=part2 /disk2 xfs discard,nofail,defaults 0 2 . lik zum repository noch!!","title":"5. Persistent mounting"},{"location":"Exercises/Ex20/","text":"Exercise 20","title":"Exercise 20"},{"location":"Exercises/Ex20/#exercise-20","text":"","title":"Exercise 20"},{"location":"Exercises/Ex28/","text":"Exercise 28 Goal Create two hosts that are connected via a private network One of them is also connected to the public internet via a public ip address Steps 1. Create private net resource \"hcloud_network\" \"privateNet\" { name = \"Private network\" ip_range = \"10.0.0.0/8\" } 2. Create private subnet resource \"hcloud_network_subnet\" \"privateSubnet\" { network_id = hcloud_network.privateNet.id type = \"cloud\" network_zone = \"eu-central\" ip_range = \"10.0.1.0/24\" } 3. Create route to gateway for all packages resource \"hcloud_network_route\" \"gateway\"{ network_id = hcloud_network.privateNet.id destination = \"0.0.0.0/0\" gateway = \"10.0.1.20\" } 4. Create primary ip address for gateway vm 5. Create gateway vm and connect to primary ip resource \"hcloud_server\" \"gateway\" { ... public_net { ipv4 = hcloud_primary_ip.gatwewayIp.id } network { network_id = hcloud_network.privateNet.id ip = \"10.0.1.20\" } } 6. Create internal vm without public ip address resource \"hcloud_server\" \"intern\" { .... public_net { ipv4_enabled = false ipv6_enabled = false } network { network_id = hcloud_network.privateNet.id ip = \"10.0.1.30\" } } 7. Both host get the same ssh configuration with the same authorized_keys via cloud-init (host hopping allows for chained ssh connections) 8. Gateway gets firewall attached to allow ssh connections 9. chained ssh connection fails because cloud-init fails because it has no internet 10. install apt-cache-ng on gateway and setup so packages updates are possible 11. setup intern host to use apt-cache-ng to update packages Result files","title":"Exercise 28"},{"location":"Exercises/Ex28/#exercise-28","text":"","title":"Exercise 28"},{"location":"Exercises/Ex28/#goal","text":"Create two hosts that are connected via a private network One of them is also connected to the public internet via a public ip address","title":"Goal"},{"location":"Exercises/Ex28/#steps","text":"","title":"Steps"},{"location":"Exercises/Ex28/#1-create-private-net","text":"resource \"hcloud_network\" \"privateNet\" { name = \"Private network\" ip_range = \"10.0.0.0/8\" }","title":"1. Create private net"},{"location":"Exercises/Ex28/#2-create-private-subnet","text":"resource \"hcloud_network_subnet\" \"privateSubnet\" { network_id = hcloud_network.privateNet.id type = \"cloud\" network_zone = \"eu-central\" ip_range = \"10.0.1.0/24\" }","title":"2. Create private subnet"},{"location":"Exercises/Ex28/#3-create-route-to-gateway-for-all-packages","text":"resource \"hcloud_network_route\" \"gateway\"{ network_id = hcloud_network.privateNet.id destination = \"0.0.0.0/0\" gateway = \"10.0.1.20\" }","title":"3. Create route to gateway for all packages"},{"location":"Exercises/Ex28/#4-create-primary-ip-address-for-gateway-vm","text":"","title":"4. Create primary ip address for gateway vm"},{"location":"Exercises/Ex28/#5-create-gateway-vm-and-connect-to-primary-ip","text":"resource \"hcloud_server\" \"gateway\" { ... public_net { ipv4 = hcloud_primary_ip.gatwewayIp.id } network { network_id = hcloud_network.privateNet.id ip = \"10.0.1.20\" } }","title":"5. Create gateway vm and connect to primary ip"},{"location":"Exercises/Ex28/#6-create-internal-vm-without-public-ip-address","text":"resource \"hcloud_server\" \"intern\" { .... public_net { ipv4_enabled = false ipv6_enabled = false } network { network_id = hcloud_network.privateNet.id ip = \"10.0.1.30\" } }","title":"6. Create internal vm without public ip address"},{"location":"Exercises/Ex28/#7-both-host-get-the-same-ssh-configuration-with-the-same-authorized_keys-via-cloud-init-host-hopping-allows-for-chained-ssh-connections","text":"","title":"7. Both host get the same ssh configuration with the same authorized_keys via cloud-init (host hopping allows for chained ssh connections)"},{"location":"Exercises/Ex28/#8-gateway-gets-firewall-attached-to-allow-ssh-connections","text":"","title":"8. Gateway gets firewall attached to allow ssh connections"},{"location":"Exercises/Ex28/#9-chained-ssh-connection-fails-because-cloud-init-fails-because-it-has-no-internet","text":"","title":"9. chained ssh connection fails because cloud-init fails because it has no internet"},{"location":"Exercises/Ex28/#10-install-apt-cache-ng-on-gateway-and-setup-so-packages-updates-are-possible","text":"","title":"10. install apt-cache-ng on gateway and setup so packages updates are possible"},{"location":"Exercises/Ex28/#11-setup-intern-host-to-use-apt-cache-ng-to-update-packages","text":"","title":"11. setup intern host to use apt-cache-ng to update packages"},{"location":"Exercises/Ex28/#result","text":"files","title":"Result"},{"location":"Exercises/Ex29/","text":"Exercise 28 Goal Create two hosts that are connected via a private network One of them is also connected to the public internet via a public ip address Steps 1. install and configure apt-cache-ng on gateway add package to cloud-init configure with \"PassTroughPattern: .*\" in acng.conf 2. install and configure apt-cache-ng on client Result files","title":"Exercise 28"},{"location":"Exercises/Ex29/#exercise-28","text":"","title":"Exercise 28"},{"location":"Exercises/Ex29/#goal","text":"Create two hosts that are connected via a private network One of them is also connected to the public internet via a public ip address","title":"Goal"},{"location":"Exercises/Ex29/#steps","text":"","title":"Steps"},{"location":"Exercises/Ex29/#1-install-and-configure-apt-cache-ng-on-gateway","text":"add package to cloud-init configure with \"PassTroughPattern: .*\" in acng.conf","title":"1. install and configure apt-cache-ng on gateway"},{"location":"Exercises/Ex29/#2-install-and-configure-apt-cache-ng-on-client","text":"","title":"2. install and configure apt-cache-ng on client"},{"location":"Exercises/Ex29/#result","text":"files","title":"Result"},{"location":"Exercises/test/test/","text":"","title":"Test"},{"location":"glossar/glos/","text":"Word Definition ssh S ecure Sh ell","title":"Glos"}]}